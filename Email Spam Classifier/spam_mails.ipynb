{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting whether the email is spam or not\n",
    "\n",
    "\n",
    "#### 1. Problem Definition\n",
    "In a statement,\n",
    "The problem we will be exploring is binary classification (a sample can only be one of two things).\n",
    "\n",
    "This is because we have to build a system to identify spam emails(either the mail is spam or not).\n",
    "\n",
    "#### 2. Data\n",
    "All of the dataset values were provided by a company.\n",
    "\n",
    "#### 3. Evaluation\n",
    "Evaluating a models predictions using problem-specific evaluation metrics.\n",
    "Following are the metrics we used to evaluate the performance of ML techniques:\n",
    "\n",
    "1. Precision\n",
    "Precision refers to the closeness of two or more measurements to each other. In Machine Learning, precision is the fraction of relevant instances among the retrieved instances. Precision = TP / (TP + FP) (Where TP = True Positive, TN = True Negative, FP = False Positive, FN = False Negative).\n",
    "\n",
    "2. Accuracy\n",
    "Accuracy refers to the closeness of a measured value to a standard or known value. Accuracy = (TP+TN) / ALL\n",
    "\n",
    "3. Recall\n",
    "Recall is how many of the true positives were recalled (found), i.e. how many of the correct hits were also found. Recall = TP / (TP + FN)\n",
    "\n",
    "4. F1-Score\n",
    "F1-scores are a statistical method for determining accuracy accounting for both precision and recall. It is essentially the harmonic mean of precision and recall.\n",
    "\n",
    "5. AUC\n",
    "AUC is the area under the ROC curve. The closer the AUC value is to 1, the better the model.\n",
    "\n",
    "#### 4. Features\n",
    "The following are the features we'll use to predict our target variable Is_Response.\n",
    "\n",
    "Features:                    \n",
    "1) subject :subject of the e-mail.                    \n",
    "2) message :contains the body of the email .             \n",
    "\n",
    "Label:       \n",
    "1) label: 0 or 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular EDA (exploratory data analysis) and plotting libraries\n",
    "!pip install imblearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn import under_sampling, over_sampling\n",
    "# Models from Scikit-Learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Model Evaluations\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve,roc_auc_score,auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the csv files\n",
    "df_sms =pd.read_csv(\"spam.csv\")\n",
    "df_sms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display The head ==> To Check if Data is Properly Imported\n",
    "df_sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chechking the features,duplicate values and nan values in the Datasets\n",
    "\n",
    "print(\"\\nFeatures/Columns : \\n\", df_sms.columns)\n",
    "print(\"\\n\\nNumber of Features/Columns : \", len(df_sms.columns))\n",
    "print(\"\\nNumber of Rows : \",len(df_sms))\n",
    "print(\"\\n\\nData Types :\\n\", df_sms.dtypes)\n",
    "print(\"sum of duplicated values{}\\n\".format(df_sms.duplicated().sum()))\n",
    "print(\"\\nContains NaN/Empty cells : \", df_sms.isnull().values.any())\n",
    "print(\"\\nTotal empty cells by column :\\n\", df_sms.isnull().sum(),\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing NaN values in subject field with mode text\n",
    "df_sms['subject'].fillna(df_sms['subject'].mode()[0], inplace=True)\n",
    "df_sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new column message_new which contains combine text of  both subject and message \n",
    "df_sms['message_new'] = df_sms[['subject', 'message']].apply(lambda x: ' '.join(x), axis = 1)\n",
    "df_sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* subject field contains important information so combining it with messaget field and assign to message_new field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting subject,message Column\n",
    "df_sms.drop(columns=['subject','message'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the head\n",
    "df_sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('ham and spam counts','\\n',df_sms.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ratio\n",
    "print ('ham ratio = ', round(len(df_sms[df_sms['label']== 0]) / len(df_sms.label),2)*100,'%')\n",
    "print ('spam ratio  = ', round(len(df_sms[df_sms['label']==1]) / len(df_sms.label),2)*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check the count of each class in target varaible\n",
    "%matplotlib inline\n",
    "sns.catplot(x='label',data=df_sms,kind='count',hue='label')\n",
    "plt.legend()\n",
    "plt.xlabel(\"0 = Ham , 1 = Spam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we see data is unbalanced but all the classes have significant number of instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets describe \n",
    "df_sms.groupby('label').describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New column for Length of message\n",
    "df_sms['length'] = df_sms.message_new.str.len()\n",
    "df_sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace email addresses with 'email'\n",
    "df_sms['message_new'] = df_sms['message_new'].str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$',\n",
    "                                 'emailaddress')\n",
    "\n",
    "# Replace URLs with 'webaddress'\n",
    "df_sms['message_new'] = df_sms['message_new'].str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$',\n",
    "                                  'webaddress')\n",
    "\n",
    "# Replace money symbols with 'moneysymb' (£ can by typed with ALT key + 156)\n",
    "df_sms['message_new'] = df_sms['message_new'].str.replace(r'£|\\$', 'dollers')\n",
    "    \n",
    "# Replace 10 digit phone numbers (formats include paranthesis, spaces, no spaces, dashes) with 'phonenumber'\n",
    "df_sms['message_new'] = df_sms['message_new'].str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$',\n",
    "                                  'phonenumber')\n",
    "\n",
    "    \n",
    "# Replace numbers with 'numbr'\n",
    "df_sms['message_new'] = df_sms['message_new'].str.replace(r'\\d+(\\.\\d+)?', 'numbr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Return the wordnet object value corresponding to the POS tag\n",
    "#Part-Of-Speech (POS) tagging: assign a tag to every word to define if it corresponds to a noun, a verb etc. using the WordNet lexical database\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # lower text\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[^\\w\\s]\", \" \", text) \n",
    "    # tokenize text and remove puncutation\n",
    "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    "    # remove words that contain numbers\n",
    "    text = [word for word in text if not any(c.isdigit() for c in word)]\n",
    "    # Remove leading and trailing whitespace\n",
    "    #text=re.sub(\"[^\\s+|\\s+?$]\",\" \",text)\n",
    "    # remove stop words\n",
    "    stop = set(stopwords.words('english') + ['u', 'ü', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure'])\n",
    "    text = [x for x in text if x not in stop]\n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "    # pos tag text\n",
    "    pos_tags = pos_tag(text)\n",
    "    # lemmatize text\n",
    "    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n",
    "    #text=stemmer.stem(text)\n",
    "    # remove words with only two letter\n",
    "    text = [t for t in text if len(t) > 2]\n",
    "    # join all\n",
    "    text = \" \".join(text)\n",
    "    return(text)\n",
    "\n",
    "# clean text data\n",
    "df_sms[\"message_clean\"] = df_sms[\"message_new\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To clean textual data, call  custom 'clean_text' function that performs several transformations:\n",
    "\n",
    "* lower the text\n",
    "* tokenize the text (split the text into words) and remove the punctuation\n",
    "* remove useless words that contain numbers\n",
    "* remove useless stop words like 'the', 'a' ,'this','is' etc.\n",
    "* Part-Of-Speech (POS) tagging: assign a tag to every word to define if it corresponds to a noun, a verb etc. using the WordNet   lexical database\n",
    "* lemmatize the text: transform every word into their root form (e.g. rooms -> room, slept -> sleep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add number of characters column\n",
    "df_sms[\"clean_length\"] = df_sms[\"message_clean\"].apply(lambda x: len(x))\n",
    "\n",
    "# Add number of words column\n",
    "df_sms[\"clean_words\"] = df_sms[\"message_clean\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "df_sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Total length removal\n",
    "print ('Origian Length', df_sms.length.sum())\n",
    "print ('Clean Length', df_sms.clean_length.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Message distribution BEFORE cleaning\n",
    "f,ax = plt.subplots(1,2,figsize = (15,8))\n",
    "\n",
    "sns.distplot(df_sms[df_sms['label']==1]['length'],bins=20,ax=ax[0],label='Spam messages distribution',color='r')\n",
    "\n",
    "ax[0].set_xlabel('Spam sms length')\n",
    "ax[0].legend()\n",
    "\n",
    "sns.distplot(df_sms[df_sms['label']==0]['length'],bins=20,ax=ax[1],label='ham messages distribution')\n",
    "ax[1].set_xlabel('ham sms length')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Message distribution AFTER cleaning\n",
    "f,ax = plt.subplots(1,2,figsize = (15,8))\n",
    "\n",
    "sns.distplot(df_sms[df_sms['label']==1]['clean_length'],bins=20,ax=ax[0],label='Spam messages distribution',color='r')\n",
    "ax[0].set_xlabel('Spam sms length')\n",
    "ax[0].legend()\n",
    "\n",
    "sns.distplot(df_sms[df_sms['label']==0]['clean_length'],bins=20,ax=ax[1],label='ham messages distribution')\n",
    "ax[1].set_xlabel('ham sms length')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After clean the  text,it almost looks  normal distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting sense of loud words in spam \n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "spams = df_sms['message_clean'][df_sms['label']==1]\n",
    "\n",
    "spam_cloud = WordCloud(width=700,height=500,background_color='white',max_words=50).generate(' '.join(spams))\n",
    "\n",
    "plt.figure(figsize=(10,8),facecolor='r')\n",
    "plt.imshow(spam_cloud)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Most of the words are indeed related to the spam are: dollers,free,offer,business,money etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting sense of loud words in ham \n",
    "\n",
    "hams = df_sms['message_clean'][df_sms['label']==0]\n",
    "spam_cloud = WordCloud(width=600,height=400,background_color='white',max_words=50).generate(' '.join(hams))\n",
    "plt.figure(figsize=(10,8),facecolor='k')\n",
    "plt.imshow(spam_cloud)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Most of the words are indeed related to the ham are: language,linguistic,fax etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building word dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#lets count most frequent words in spam\n",
    "count_spam = Counter(\" \".join(df_sms[df_sms['label']==1][\"message_clean\"]).split()).most_common(20)\n",
    "df_count_spam = pd.DataFrame.from_dict(count_spam)\n",
    "df_count_spam = df_count_spam.rename(columns={0: \"words of spam\", 1 : \"count\"})\n",
    "df_count_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets count most frequent words in ham\n",
    "\n",
    "count_ham = Counter(\" \".join(df_sms[df_sms['label']==0][\"message_clean\"]).split()).most_common(20)\n",
    "df_count_ham = pd.DataFrame.from_dict(count_ham)\n",
    "df_count_ham = df_count_ham.rename(columns={0: \"words of ham\", 1 : \"count\"})\n",
    "df_count_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets plot most common 20 words of ham\n",
    "plt.figure(figsize=(17,10))\n",
    "sns.barplot(x='words of ham',y=\"count\",data=df_count_ham)\n",
    "plt.title('Top 20 words of ham')\n",
    "plt.xlabel('ham words')\n",
    "plt.ylabel('Count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets plot most common 20 words of spam\n",
    "plt.figure(figsize=(17,10))\n",
    "sns.barplot(x='words of spam',y=\"count\",data=df_count_spam)\n",
    "plt.title('Top 20 words of spam')\n",
    "plt.xlabel('spam words')\n",
    "plt.ylabel('Count')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to apply the count vectorizer(BOW)\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "def features_transform_cv(mail):\n",
    "    #get the bag of words for the mail text\n",
    "    cv = CountVectorizer()\n",
    "    features = cv.fit_transform(mail)\n",
    "    #print sparsity value\n",
    "    print('sparse matrix shape:', features.shape)\n",
    "    print('number of non-zeros:', features.nnz) \n",
    "    print('sparsity: %.2f%%' % (100.0 * features.nnz / (features.shape[0] * features.shape[1])))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=features_transform_cv(df_sms['message_clean'])\n",
    "y = df_sms['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#        Function which will find best Random State and then calculate Maximum Accuracy Score corresponding to it\n",
    "#                                  and print accuracy score in one go.\n",
    "def max_acc_score(regr,x,y):\n",
    "    max_acc_score=0\n",
    "    final_r_state=0\n",
    "    for r_state in range(42,100):\n",
    "        x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.20,random_state=r_state,stratify=y)\n",
    "        x_train, y_train = SMOTE().fit_sample(x_train, y_train)\n",
    "        regr.fit(x_train,y_train)\n",
    "        y_pred=regr.predict(x_test)\n",
    "        acc_score=accuracy_score(y_test,y_pred)\n",
    "        if acc_score > max_acc_score:\n",
    "            max_acc_score=acc_score\n",
    "            final_r_state=r_state\n",
    "    print('Max Accuracy Score corresponding to Random State ', final_r_state, 'is:', max_acc_score)\n",
    "    print('\\n')\n",
    "    return final_r_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lists to store model name, Accuracy score, cross_val_score, Auc Roc score \n",
    "models =[]\n",
    "models.append((\"Random Forest\",RandomForestClassifier()))\n",
    "models.append((\"GradientBoostingClassifier\",GradientBoostingClassifier()))    \n",
    "models.append((\"Logistic Regression\",LogisticRegression()))\n",
    "models.append(('MultinomialNB',MultinomialNB()))\n",
    "models.append((\"SVC\",SVC()))\n",
    "models.append((\"DecisionTree\",DecisionTreeClassifier()))\n",
    "models.append(('AdaBoostClassifier',AdaBoostClassifier()))\n",
    "\n",
    "\n",
    "#     Lists to store model name, Learning score, Accuracy score, cross_val_score, Auc Roc score .\n",
    "Model=[]\n",
    "Score=[]\n",
    "Acc_score=[]\n",
    "cvs=[]\n",
    "rocscore=[]\n",
    "#            For Loop to Calculate Accuracy Score, Cross Val Score, Classification Report, Confusion Matrix\n",
    "\n",
    "for name,model in models:\n",
    "    print('***************************',name,'*****************************')\n",
    "    print('\\n')\n",
    "    Model.append(name)\n",
    "    print(model)\n",
    "    print('\\n')\n",
    "    \n",
    "     #        Now here I am calling a function which will calculate the max accuracy score for each model \n",
    "     #                               and return best random state.\n",
    "    r_state=max_acc_score(model,x,y)\n",
    "    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.20,random_state=r_state,stratify=y)\n",
    "    x_train, y_train = SMOTE().fit_sample(x_train, y_train)\n",
    "    model.fit(x_train,y_train)\n",
    "#..............Learning Score...........\n",
    "    score=model.score(x_train,y_train)\n",
    "    print('Learning Score : ',score)\n",
    "    Score.append(score*100)\n",
    "    y_pred=model.predict(x_test)\n",
    "    acc_score=accuracy_score(y_test,y_pred)\n",
    "    print('Accuracy Score : ',acc_score)\n",
    "    Acc_score.append(acc_score*100)\n",
    "#................Finding Cross_val_score..................    \n",
    "    cv_score=cross_val_score(model,x,y,cv=10,scoring='f1').mean()\n",
    "    print('Cross Val Score : ', cv_score)\n",
    "    cvs.append(cv_score*100)\n",
    "    \n",
    "#................Roc auc score...........................    \n",
    "    false_positive_rate,true_positive_rate, thresholds=roc_curve(y_test,y_pred)\n",
    "    roc_auc=auc(false_positive_rate, true_positive_rate)\n",
    "    print('roc auc score : ', roc_auc)\n",
    "    rocscore.append(roc_auc*100)\n",
    "    print('\\n')\n",
    "    print('Classification Report:\\n',classification_report(y_test,y_pred))\n",
    "    print('\\n')\n",
    "    print('Confusion Matrix:\\n',confusion_matrix(y_test,y_pred))\n",
    "    print('\\n')\n",
    "    plt.figure(figsize=(10,40))\n",
    "    plt.subplot(911)\n",
    "    plt.title(name)\n",
    "    plt.plot(false_positive_rate,true_positive_rate,label='AUC = %0.2f'% roc_auc)\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('True_positive_rate')\n",
    "    plt.xlabel('False_positive_rate')\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a Dataframe comprises of Differnt Calculated Scores :\n",
    "result=pd.DataFrame({'Model': Model,'Learning Score': Score,'Accuracy Score': Acc_score,'Cross Val Score':cvs,\n",
    "                     'Roc_Auc_curve':rocscore}) \n",
    "result.style.background_gradient(cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting bar plot of accuracy scores and crossval scores of various models\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.subplot(3,2,1)\n",
    "sns.barplot(x = 'Accuracy Score', y = 'Model', data = result)\n",
    "\n",
    "plt.subplot(3,2,2)\n",
    "sns.barplot(x = 'Cross Val Score', y = 'Model', data = result)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to calculate accuracy,precision,recall and f1 score\n",
    "def evaluate_preds(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    Performs evaluation comparison on y_true labels vs. y_pred labels.\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_preds)\n",
    "    precision = precision_score(y_true, y_preds)\n",
    "    recall = recall_score(y_true, y_preds)\n",
    "    f1 = f1_score(y_true, y_preds)\n",
    "    metric_dict = {\"accuracy\": round(accuracy, 2),\n",
    "                   \"precision\": round(precision, 2), \n",
    "                   \"recall\": round(recall, 2),\n",
    "                   \"f1\": round(f1, 2)}\n",
    "    print(f\"Acc: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 score: {f1:.2f}\")\n",
    "\n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From above results we can conclude that MultinomialNB performing well as compared to other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets make our MultinomialNB as final model\n",
    "x_train,x_test,y_train,y_test=train_test_split(x, y,random_state = 47,test_size=0.20,stratify=y)\n",
    "x_train, y_train = SMOTE().fit_sample(x_train, y_train)\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(x_train,y_train)\n",
    "predmnb=mnb.predict(x_test)\n",
    "evaluate_preds(y_test,predmnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with probabilities\n",
    "y_probs = mnb.predict_proba(x_test)\n",
    "\n",
    "# Keep the probabilites of the positive class only\n",
    "y_probs = y_probs[:, 1]\n",
    "\n",
    "# Calculate fpr, tpr and thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "\n",
    "# Check the false positive rate\n",
    "fpr\n",
    "plt.plot([0,1],[0,1])\n",
    "plt.plot(fpr,tpr,label='MultinomialNB')\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('MultinomialNB')\n",
    "plt.show()\n",
    "print('roc_auc_score = ',roc_auc_score(y_test, y_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "\n",
    "def plot_conf_mat(y_test, predmnb):\n",
    "    \"\"\"\n",
    "    Plots a nice looking confusion matrix using Seaborn's heatmap()\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax = sns.heatmap(confusion_matrix(y_test, predmnb),\n",
    "                     annot=True,\n",
    "                     cbar=False)\n",
    "    plt.xlabel(\"Test label\")\n",
    "    plt.ylabel(\"Predicted label\")\n",
    "    \n",
    "    bottom, top = ax.get_ylim()\n",
    "    ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "    \n",
    "plot_conf_mat(y_test, predmnb)\n",
    "#lets printed confusion_matrix\n",
    "print(confusion_matrix(y_test, predmnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing predicted values \n",
    "test=pd.DataFrame(data=y_test,)\n",
    "test['Predicted values']=predmnb\n",
    "test\n",
    "# On the lest side values are those fields which are taken by machine for test..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Predicted values in csv file\n",
    "df1=pd.DataFrame(predmnb)\n",
    "df1.to_csv('Project_2_spam_mails_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets save the model using joblib\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(mnb,'Project_2_spam_mails.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the file using joblib\n",
    "mnb_from_joblib=joblib.load('Project_2_spam_mails.pkl')\n",
    "joblib_y_preds = mnb_from_joblib.predict(x_test)\n",
    "evaluate_preds(y_test, joblib_y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "1. * Some of the words like dollers,free,offer,business,money etc\" are indication of spams\n",
    "2. * MultinomialNB performing very well as compared to other models considering below results :\n",
    "       >> * Precision: 1\n",
    "       >> * Recall: 1\n",
    "       >> * F1 score: 1\n",
    "       >> * roc_auc_score : 1\n",
    "       >> * accuracy : 100%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
